{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "# ECE521 Assignment 2\n",
    "# Shafquat Arefeen\n",
    "# Ahmed Faraz Khan 998822779"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1455575979\n"
     ]
    }
   ],
   "source": [
    "# Random seed - current time\n",
    "seed_time = int((datetime.datetime.now()-datetime.datetime(1970,1,1)).total_seconds())\n",
    "np.random.seed(seed_time)\n",
    "print \"Random Seed\", seed_time\n",
    "\n",
    "with np.load(\"notMNIST.npz\") as data:\n",
    "    images, labels = data[\"images\"], data[\"labels\"]\n",
    "images.shape\n",
    "\n",
    "# Parameters\n",
    "\n",
    "# Constant parameters\n",
    "n_input = 784 # Input data dimension\n",
    "n_classes = 10 # Output classes\n",
    "training_set_size = 15000 \n",
    "dropout_rate = 0.5\n",
    "display_step = 1\n",
    "n_units = [n_input] # include at least the input \n",
    "\n",
    "# Less constant parameters\n",
    "batch_size = 1000\n",
    "num_epochs = 100\n",
    "momentum = 0.99\n",
    "\n",
    "random_hyperparameters = False\n",
    "\n",
    "# NN Parameters\n",
    "learning_rate = 0.1\n",
    "n_layers = 1\n",
    "n_hidden1 = 1000 # Number of hidden units in layer 1\n",
    "n_hidden2 = 500 # Number of hidden units in layer 2\n",
    "\n",
    "\n",
    "# Manual hyperparameters\n",
    "if random_hyperparameters == False:\n",
    "    n_units += [n_hidden1]\n",
    "    if n_layers == 2:\n",
    "        n_units += [n_hidden2]\n",
    "    n_units += [n_classes] # include output layer\n",
    "\n",
    "# Random hyperparameter sampling - note: random_integers(low,high) gives low <= x <= high\n",
    "elif random_hyperparameters == True:\n",
    "    #n_units = [n_input]\n",
    "    n_layers = np.random.randon_integers(1,3) # Random number of layers, between 1 and 3\n",
    "    for i in range(0, n_layers):\n",
    "        n_units += [np.random.random_integers(100,500)] # Random number of hidden units per layer\n",
    "    n_units += [n_classes]\n",
    "    isDropout = bool(np.random.random_integers(0,1))\n",
    "    log_learning_rate = np.random.random_integers(-4,-2)\n",
    "    learning_rate = np.exp(log_learning_rate)\n",
    "\n",
    "\n",
    "def rearrangeImages(image_array):\n",
    "    rearranged = np.zeros((image_array.shape[2], image_array.shape[0] * image_array.shape[1]), dtype=np.float32)\n",
    "    for num in range(0, image_array.shape[2]):\n",
    "        flat_image = image_array[:,:,num].flatten()\n",
    "        max_elem = np.amax(flat_image)\n",
    "        for i in range(0, len(flat_image)):\n",
    "            flat_image[i] = flat_image[i] / max_elem\n",
    "        rearranged[num,:] = flat_image\n",
    "    return rearranged\n",
    "                \n",
    "    \n",
    "def oneHot(labels, vector_size):\n",
    "    oneHot = np.zeros((len(labels), vector_size))\n",
    "    for i in range(0, len(labels)):\n",
    "        label_index = labels[i]\n",
    "        oneHot[i, label_index] = 1\n",
    "    return oneHot\n",
    "\n",
    "def preprocess_data(images, labels):\n",
    "    # Create one-hot label vectors and flatten images\n",
    "    one_hot_labels = oneHot(labels, 10)\n",
    "    flat_images = rearrangeImages(images)\n",
    "    return flat_images, one_hot_labels\n",
    "\n",
    "def neural_network(_X, _weights, _biases, n_layers): # 1 <= n_layers\n",
    "    # Hidden units using a ReLU activation function\n",
    "    hidden_layers = {} \n",
    "    hidden_layers_drop = {}\n",
    "    \n",
    "    # Always include at least one layer\n",
    "    hidden_layers[1] = tf.nn.relu(tf.add(tf.matmul(_X, _weights[1]), _biases[1]))\n",
    "    if (n_layers > 1):\n",
    "        for i in range(2, n_layers + 1):\n",
    "            hidden_layers[i] = tf.nn.relu(tf.add(tf.matmul(hidden_layers[i-1], _weights[i]), _biases[i]))\n",
    "            keep_prob = tf.placeholder(tf.float32)\n",
    "            hidden_layers_drop[i] = tf.nn.dropout(hidden_layers[i], keep_prob)\n",
    "    return tf.matmul(hidden_layers_drop[n_layers], _weights[n_layers + 1]) + _biases[n_layers + 1]\n",
    "    #if (n_layers == 1):\n",
    "        #return tf.matmul(hidden_layer1, _weights['out']) + _biases['out']\n",
    "    #elif (n_layers == 2):\n",
    "        #hidden_layer2 = tf.nn.relu(tf.add(tf.matmul(hidden_layer1, _weights['h2']), _biases['b2']))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Separate training, validation and test sets\n",
    "flat_images, one_hot_labels = preprocess_data(images, labels)\n",
    "training_labels = one_hot_labels[0:15000]\n",
    "validation_labels = one_hot_labels[15000:16000]\n",
    "testing_labels = one_hot_labels[16000:]\n",
    "\n",
    "training_images = flat_images[:15000,:]\n",
    "validation_images = flat_images[15000:16000,:]\n",
    "testing_images = flat_images[16000:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Placeholder Inputs\n",
    "X = tf.placeholder(\"float32\", shape=(None, 28*28)) # 28x28 flattened arrays\n",
    "Y = tf.placeholder(\"float32\", shape=(None, 10)) # 10-class classifier\n",
    "\n",
    "# Variables\n",
    "W = tf.Variable(np.random.randn(28*28, 10).astype(\"float32\"), name=\"weight\")\n",
    "b = tf.Variable(np.random.randn(10).astype(\"float32\"), name=\"bias\")\n",
    "\n",
    "logits = tf.add(tf.matmul(X, W), b)\n",
    "output = tf.nn.softmax(logits)\n",
    "\n",
    "\n",
    "cost_batch = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, targets=Y)\n",
    "cost = tf.reduce_mean(cost_batch)\n",
    "\n",
    "# Calculate cost (cross-entropy)\n",
    "#Y_ = tf.placeholder(tf.float32, shape=(None, 10))\n",
    "#cross_entropy = -tf.reduce_sum(Y_*tf.log(Y))\n",
    "\n",
    "# Gradient descent\n",
    "train_op = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum).minimize(cost)\n",
    "\n",
    "# Prediction for multiple outputs\n",
    "#pred = tf.greater(output, 0.5)\n",
    "#pred_float = tf.cast(pred, \"float\")\n",
    "\n",
    "# Accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(Y,1), tf.argmax(output,1))#tf.argmax(Y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "norm_w = tf.nn.l2_loss(W)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  for i in xrange(int(FLAGS.training_set_size/batch_size)):\n",
    "    x_batch = training_images[i * batch_size: (i + 1) * batch_size]\n",
    "    y_batch = training_labels[i * batch_size: (i + 1) * batch_size]\n",
    "    cost_np, _ = sess.run([cost, train_op],\n",
    "                          feed_dict={X: x_batch, Y: y_batch})\n",
    "    \n",
    "    #Display logs per epoch step\n",
    "  if (epoch % display_step) == 0:\n",
    "    cost_train, accuracy_train = sess.run([cost, accuracy],feed_dict={X: training_images, Y: training_labels})\n",
    "    cost_eval, accuracy_eval, norm_w_np = sess.run([cost, accuracy, norm_w],\n",
    "                                                   feed_dict={X: validation_images, Y: validation_labels})    \n",
    "    print (\"Epoch:%04d, cost=%0.9f, Train Accuracy=%0.4f, Eval Accuracy=%0.4f,    Norm of Weights=%0.4f\" %\n",
    "           (epoch+1, cost_train, accuracy_train, accuracy_eval, norm_w_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001\n",
      "Minibatch accuracy: 0.29800\n",
      "Validation Accuracy: 0.29900\n",
      "Epoch: 0002\n",
      "Minibatch accuracy: 0.35900\n",
      "Validation Accuracy: 0.38800\n",
      "Epoch: 0003\n",
      "Minibatch accuracy: 0.44400\n",
      "Validation Accuracy: 0.46900\n",
      "Epoch: 0004\n",
      "Minibatch accuracy: 0.53600\n",
      "Validation Accuracy: 0.54000\n",
      "Epoch: 0005\n",
      "Minibatch accuracy: 0.60600\n",
      "Validation Accuracy: 0.59600\n",
      "Epoch: 0006\n",
      "Minibatch accuracy: 0.65500\n",
      "Validation Accuracy: 0.65300\n",
      "Epoch: 0007\n",
      "Minibatch accuracy: 0.69900\n",
      "Validation Accuracy: 0.67900\n",
      "Epoch: 0008\n",
      "Minibatch accuracy: 0.73100\n",
      "Validation Accuracy: 0.69200\n",
      "Epoch: 0009\n",
      "Minibatch accuracy: 0.75000\n",
      "Validation Accuracy: 0.71200\n",
      "Epoch: 0010\n",
      "Minibatch accuracy: 0.76500\n",
      "Validation Accuracy: 0.72700\n",
      "Epoch: 0011\n",
      "Minibatch accuracy: 0.77800\n",
      "Validation Accuracy: 0.73400\n",
      "Epoch: 0012\n",
      "Minibatch accuracy: 0.78700\n",
      "Validation Accuracy: 0.74500\n",
      "Epoch: 0013\n",
      "Minibatch accuracy: 0.79100\n",
      "Validation Accuracy: 0.75700\n",
      "Epoch: 0014\n",
      "Minibatch accuracy: 0.79800\n",
      "Validation Accuracy: 0.76200\n",
      "Epoch: 0015\n",
      "Minibatch accuracy: 0.80700\n",
      "Validation Accuracy: 0.76400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-9ad996310e4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mbatch_ys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mtf_train_dataset\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;31m# Display per epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munique_fetch_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m       return tf_session.TF_Run(self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 428\u001b[1;33m                                target_list)\n\u001b[0m\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Task 2: Neural Network Training\n",
    "\n",
    "# tf Graph input\n",
    "#x = tf.placeholder(\"float\", [None, n_input])\n",
    "#y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, n_input))\n",
    "tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, n_classes))\n",
    "tf_valid_dataset = tf.constant(validation_images, dtype=tf.float32, shape = validation_images.shape)\n",
    "tf_test_dataset = tf.constant(testing_images, dtype=tf.float32, shape = testing_images.shape)\n",
    "\n",
    "pred = tf.placeholder(tf.float32, shape=[None, n_classes])\n",
    "label = tf.placeholder(tf.float32, shape=[None, n_classes])\n",
    "\n",
    "# Store layers' weights & biases\n",
    "weights = {}\n",
    "biases = {}\n",
    "for l in range(1, n_layers + 2):\n",
    "    # Normalize by the number of input units\n",
    "    weights[l] = tf.Variable(tf.random_normal([n_units[l-1], n_units[l]], stddev = np.sqrt(1.0/n_units[l-1]) ))\n",
    "    biases[l] = tf.Variable(tf.random_normal([n_units[l]], stddev = np.sqrt(1.0/n_units[l-1]) ))\n",
    "\n",
    "# Construct model\n",
    "logits = neural_network(tf_train_dataset, weights, biases, n_layers)\n",
    "\n",
    "# Define cost and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, targets=tf_train_labels)) # Match pred with y\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)# momentum=momentum)\n",
    "#train_op = optimizer.minimize(cost)\n",
    "\n",
    "# Make prediction\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "valid_prediction = tf.nn.softmax(neural_network(tf_valid_dataset, weights, biases, n_layers))\n",
    "test_prediction = tf.nn.softmax(neural_network(tf_test_dataset, weights, biases, n_layers))\n",
    "\n",
    "# Compare prediction with labels to evaluate accuracy\n",
    "correct_preds = tf.equal(tf.argmax(pred, 1), tf.argmax(label, 1))\n",
    "accuracy_rate = tf.reduce_mean(tf.cast(correct_preds, \"float\"))\n",
    "    \n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost = 0.\n",
    "        num_batches = int(training_set_size/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(num_batches):\n",
    "            batch_xs = training_images[i * batch_size: (i + 1) * batch_size]\n",
    "            batch_ys = training_labels[i * batch_size: (i + 1) * batch_size]\n",
    "            feed_dict = {tf_train_dataset : batch_xs, tf_train_labels : batch_ys, keep_prob = (1.0-dropout_rate)}\n",
    "            _, l, train_preds = sess.run([train_op, cost, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        # Display per epoch \n",
    "        if (epoch % display_step) == 0:\n",
    "            train_accuracy, train_hits = sess.run([accuracy_rate, correct_preds], feed_dict={pred: train_preds, label: batch_ys, keep_prob = 1.0})\n",
    "            valid_accuracy, valid_hits  = sess.run([accuracy_rate, correct_preds], feed_dict={pred: valid_prediction.eval(), label: validation_labels, keep_prob: 1.0})\n",
    "            print \"Epoch:\", '%04d' % (epoch+1)\n",
    "            print(\"Minibatch accuracy: %.5f\" % train_accuracy)\n",
    "            print(\"Validation Accuracy: %.5f\" % valid_accuracy)\n",
    "    test_accuracy, test_hits  = sess.run([accuracy_rate, correct_preds], feed_dict={pred: test_prediction.eval(), label: testing_labels, keep_prob: 1.0})\n",
    "    print \"Training Completed\"\n",
    "    print(\"Testing Accuracy: %.5f\" % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "f = open('NN - learning rate = %f.txt' % learning_rate, 'a')\n",
    "f.write(training_accuracies)\n",
    "f.write('\\n')\n",
    "f.write(validation_accuracies)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_ys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Task 1:\n",
    "# Task 2: 5 learning rates\n",
    "# Task 3: 100, 500 and 1000 hidden units\n",
    "# Task 4: 2 layer network (500 hidden units each)\n",
    "# Task 5: Dropout\n",
    "# Task 6: Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
