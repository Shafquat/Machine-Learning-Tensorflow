{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import file_io\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Task 1: Classification Error using LR\n",
    "with np.load(\"notMNIST.npz\") as data:\n",
    "    images, labels = data[\"images\"], data[\"labels\"]\n",
    "images.shape\n",
    "\n",
    "# Parameters\n",
    "training_set_size = 15000\n",
    "#num_epochs = 100\n",
    "#learning_rate = 0.1\n",
    "batch_size = 1000\n",
    "momentum = 0.99\n",
    "display_step = 5\n",
    "\n",
    "# NN Parameters\n",
    "n_hidden = 1000 # Number of hidden units\n",
    "n_input = 784 # Input data dimension\n",
    "n_classes = 10 # Output classes\n",
    "\n",
    "def rearrangeImages(image_array):\n",
    "    rearranged = np.zeros((image_array.shape[2], image_array.shape[0] * image_array.shape[1]), dtype=np.float32)\n",
    "    for num in range(0, image_array.shape[2]):\n",
    "        flat_image = image_array[:,:,num].flatten()\n",
    "        max_elem = np.amax(flat_image)\n",
    "        for i in range(0, len(flat_image)):\n",
    "            flat_image[i] = flat_image[i] / max_elem\n",
    "        rearranged[num,:] = flat_image\n",
    "    return rearranged\n",
    "                \n",
    "    \n",
    "def oneHot(labels, vector_size):\n",
    "    oneHot = np.zeros((len(labels), vector_size))\n",
    "    for i in range(0, len(labels)):\n",
    "        label_index = labels[i]\n",
    "        oneHot[i, label_index] = 1\n",
    "    return oneHot\n",
    "\n",
    "def preprocess_data(images, labels):\n",
    "    # Create one-hot label vectors and flatten images\n",
    "    one_hot_labels = oneHot(labels, 10)\n",
    "    flat_images = rearrangeImages(images)\n",
    "    return flat_images, one_hot_labels\n",
    "\n",
    "def neural_network(_X, _weights, _biases):\n",
    "    # Hidden units using a ReLU activation function\n",
    "    hidden_layer = tf.nn.relu(tf.add(tf.matmul(_X, _weights['h']), _biases['b']))\n",
    "    return tf.matmul(hidden_layer, _weights['out']) + _biases['out']\n",
    "\n",
    "#def accuracy(prediction, label):\n",
    "    # Compare predictions with labels\n",
    "    #num_correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(label, 1))\n",
    "    #return mean_accuracy = tf.reduce_mean(tf.cast(num_correct, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Separate training, validation and test sets\n",
    "flat_images, one_hot_labels = preprocess_data(images, labels)\n",
    "training_labels = one_hot_labels[0:15000]\n",
    "validation_labels = one_hot_labels[15000:16000]\n",
    "testing_labels = one_hot_labels[16000:]\n",
    "\n",
    "training_images = flat_images[:15000,:]\n",
    "validation_images = flat_images[15000:16000,:]\n",
    "testing_images = flat_images[16000:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Task 2: Neural Network Training\n",
    "\n",
    "# tf Graph input\n",
    "#x = tf.placeholder(\"float\", [None, n_input])\n",
    "#y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "input_images = tf.placeholder(tf.float32, shape=(None, n_input))\n",
    "label = tf.placeholder(tf.float32, shape=[None, n_classes])\n",
    "\n",
    "tf_train_dataset = tf.constant(training_images, dtype=tf.float32, shape = training_images.shape)\n",
    "tf_valid_dataset = tf.constant(validation_images, dtype=tf.float32, shape = validation_images.shape)\n",
    "tf_test_dataset = tf.constant(testing_images, dtype=tf.float32, shape = testing_images.shape)\n",
    "\n",
    "pred = tf.placeholder(tf.float32, shape=[None, n_classes])\n",
    "\n",
    "# Store layers' weights & biases\n",
    "weights = {\n",
    "    'h': tf.Variable(tf.random_normal([n_input, n_hidden], stddev = np.sqrt(1.0/n_input) )),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes], stddev = np.sqrt(1.0/n_hidden) ))\n",
    "}\n",
    "biases = {\n",
    "    'b': tf.Variable(tf.random_normal([n_hidden], stddev = np.sqrt(1.0/n_input) )),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes], stddev = np.sqrt(1.0/n_hidden) ))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "logits = neural_network(input_images, weights, biases)\n",
    "\n",
    "# Define cost\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, targets=label)) # Match pred with y\n",
    "\n",
    "# Make prediction\n",
    "prediction = tf.nn.softmax(logits)\n",
    "#valid_prediction = tf.nn.softmax(neural_network(input_images, weights, biases))\n",
    "#test_prediction = tf.nn.softmax(neural_network(tf_test_dataset, weights, biases))\n",
    "\n",
    "# Compare prediction with labels to evaluate accuracy\n",
    "correct_preds = tf.equal(tf.argmax(prediction, 1), tf.argmax(label, 1))\n",
    "accuracy_rate = tf.reduce_mean(tf.cast(correct_preds, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def part_2(learning_rate, batch_size, num_epochs):\n",
    "    train_accuracy = []\n",
    "    valid_accuracy = []\n",
    "    train_cost = []\n",
    "    valid_cost = []\n",
    "    train_op = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)# momentum=momentum)\n",
    "    #train_op = optimizer.minimize(cost)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    # Launch the graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        # Training cycle\n",
    "        for epoch in range(num_epochs):\n",
    "            num_batches = int(training_set_size/batch_size)\n",
    "            # Loop over all batches\n",
    "            for i in range(num_batches):\n",
    "                batch_xs = training_images[i * batch_size: (i + 1) * batch_size]\n",
    "                batch_ys = training_labels[i * batch_size: (i + 1) * batch_size]\n",
    "                feed_dict = {input_images : batch_xs, label : batch_ys}\n",
    "                l, _ = sess.run([cost, train_op], feed_dict=feed_dict)\n",
    "            #tc = sess.run(cost, feed_dict={pred: training_images, label: training_labels})\n",
    "            tc, ta = sess.run([cost, accuracy_rate], feed_dict={input_images: training_images, label: training_labels})\n",
    "            train_accuracy.append(ta) \n",
    "            vc, va  = sess.run([cost, accuracy_rate], feed_dict={input_images: validation_images, label: validation_labels})\n",
    "            valid_accuracy.append(va)\n",
    "            #valid_cost.append(vc)\n",
    "            \n",
    "            # Display per epoch \n",
    "            if (epoch % display_step) == 0:\n",
    "                print \"Epoch:\", '%04d' % (epoch+1)\n",
    "                print(\"Minibatch accuracy: %.5f, cost: %.5f \" % (train_accuracy[epoch], tc))\n",
    "                print(\"Validation Accuracy: %.5f, cost: %.5f\" % (valid_accuracy[epoch], vc))\n",
    "        test_accuracy, test_hits  = sess.run([accuracy_rate, correct_preds], feed_dict={pred: test_prediction.eval(), label: testing_labels})\n",
    "        print \"Training Completed\"\n",
    "        print(\"Testing Accuracy: %.5f\" % test_accuracy)\n",
    "        return train_accuracy, valid_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001\n",
      "Minibatch accuracy: 0.58693, cost: 0.29352 \n",
      "Validation Accuracy: 0.60100, cost: 0.29297\n",
      "Epoch:"
     ]
    }
   ],
   "source": [
    "point5_ta, point5_va = part_2(0.5, 1000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#part_2(0.1, 1000, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#part_2(0.05, 1000, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#part_2(0.01, 1000, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#part_2(0.001, 1000, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "f = open('NN - learning rate = %f.txt' % learning_rate, 'a')\n",
    "f.write(training_accuracies)\n",
    "f.write('\\n')\n",
    "f.write(validation_accuracies)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#batch_ys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_preds[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
