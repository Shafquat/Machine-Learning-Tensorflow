{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import file_io\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Task 1: Classification Error using LR\n",
    "with np.load(\"notMNIST.npz\") as data:\n",
    "    images, labels = data[\"images\"], data[\"labels\"]\n",
    "images.shape\n",
    "\n",
    "# Parameters\n",
    "training_set_size = 15000\n",
    "#num_epochs = 100\n",
    "#learning_rate = 0.1\n",
    "#batch_size = 1000\n",
    "momentum = 0.99\n",
    "display_step = 15\n",
    "\n",
    "# NN Parameters\n",
    "n_hidden = 1000 # Number of hidden units\n",
    "n_input = 784 # Input data dimension\n",
    "n_classes = 10 # Output classes\n",
    "\n",
    "def rearrangeImages(image_array):\n",
    "    rearranged = np.zeros((image_array.shape[2], image_array.shape[0] * image_array.shape[1]), dtype=np.float32)\n",
    "    for num in range(0, image_array.shape[2]):\n",
    "        flat_image = image_array[:,:,num].flatten()\n",
    "        max_elem = np.amax(flat_image)\n",
    "        for i in range(0, len(flat_image)):\n",
    "            flat_image[i] = flat_image[i] / max_elem\n",
    "        rearranged[num,:] = flat_image\n",
    "    return rearranged\n",
    "                \n",
    "    \n",
    "def oneHot(labels, vector_size):\n",
    "    oneHot = np.zeros((len(labels), vector_size))\n",
    "    for i in range(0, len(labels)):\n",
    "        label_index = labels[i]\n",
    "        oneHot[i, label_index] = 1\n",
    "    return oneHot\n",
    "\n",
    "def preprocess_data(images, labels):\n",
    "    # Create one-hot label vectors and flatten images\n",
    "    one_hot_labels = oneHot(labels, 10)\n",
    "    flat_images = rearrangeImages(images)\n",
    "    return flat_images, one_hot_labels\n",
    "\n",
    "def neural_network(_X, _weights, _biases):\n",
    "    # Hidden units using a ReLU activation function\n",
    "    hidden_layer = tf.nn.relu(tf.add(tf.matmul(_X, _weights['h']), _biases['b']))\n",
    "    return tf.matmul(hidden_layer, _weights['out']) + _biases['out']\n",
    "\n",
    "#def accuracy(prediction, label):\n",
    "    # Compare predictions with labels\n",
    "    #num_correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(label, 1))\n",
    "    #return mean_accuracy = tf.reduce_mean(tf.cast(num_correct, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Separate training, validation and test sets\n",
    "flat_images, one_hot_labels = preprocess_data(images, labels)\n",
    "training_labels = one_hot_labels[0:15000]\n",
    "validation_labels = one_hot_labels[15000:16000]\n",
    "testing_labels = one_hot_labels[16000:]\n",
    "\n",
    "training_images = flat_images[:15000,:]\n",
    "validation_images = flat_images[15000:16000,:]\n",
    "testing_images = flat_images[16000:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training_images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Placeholder Inputs\n",
    "X = tf.placeholder(\"float32\", shape=(None, 28*28)) # 28x28 flattened arrays\n",
    "Y = tf.placeholder(\"float32\", shape=(None, 10)) # 10-class classifier\n",
    "\n",
    "# Variables\n",
    "W = tf.Variable(np.random.randn(28*28, 10).astype(\"float32\"), name=\"weight\")\n",
    "b = tf.Variable(np.random.randn(10).astype(\"float32\"), name=\"bias\")\n",
    "\n",
    "logits = tf.add(tf.matmul(X, W), b)\n",
    "output = tf.nn.softmax(logits)\n",
    "\n",
    "\n",
    "cost_batch = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, targets=Y)\n",
    "cost = tf.reduce_mean(cost_batch)\n",
    "\n",
    "# Calculate cost (cross-entropy)\n",
    "#Y_ = tf.placeholder(tf.float32, shape=(None, 10))\n",
    "#cross_entropy = -tf.reduce_sum(Y_*tf.log(Y))\n",
    "\n",
    "# Gradient descent\n",
    "train_op = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum).minimize(cost)\n",
    "\n",
    "# Prediction for multiple outputs\n",
    "#pred = tf.greater(output, 0.5)\n",
    "#pred_float = tf.cast(pred, \"float\")\n",
    "\n",
    "# Accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(Y,1), tf.argmax(output,1))#tf.argmax(Y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "norm_w = tf.nn.l2_loss(W)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  for i in xrange(int(FLAGS.training_set_size/batch_size)):\n",
    "    x_batch = training_images[i * batch_size: (i + 1) * batch_size]\n",
    "    y_batch = training_labels[i * batch_size: (i + 1) * batch_size]\n",
    "    cost_np, _ = sess.run([cost, train_op],\n",
    "                          feed_dict={X: x_batch, Y: y_batch})\n",
    "    \n",
    "    #Display logs per epoch step\n",
    "  if (epoch % display_step) == 0:\n",
    "    cost_train, accuracy_train = sess.run([cost, accuracy],feed_dict={X: training_images, Y: training_labels})\n",
    "    cost_eval, accuracy_eval, norm_w_np = sess.run([cost, accuracy, norm_w],\n",
    "                                                   feed_dict={X: validation_images, Y: validation_labels})    \n",
    "    print (\"Epoch:%04d, cost=%0.9f, Train Accuracy=%0.4f, Eval Accuracy=%0.4f,    Norm of Weights=%0.4f\" %\n",
    "           (epoch+1, cost_train, accuracy_train, accuracy_eval, norm_w_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Task 2: Neural Network Training\n",
    "\n",
    "# tf Graph input\n",
    "#x = tf.placeholder(\"float\", [None, n_input])\n",
    "#y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, n_input))\n",
    "tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, n_classes))\n",
    "tf_valid_dataset = tf.constant(validation_images, dtype=tf.float32, shape = validation_images.shape)\n",
    "tf_test_dataset = tf.constant(testing_images, dtype=tf.float32, shape = testing_images.shape)\n",
    "\n",
    "pred = tf.placeholder(tf.float32, shape=[None, n_classes])\n",
    "label = tf.placeholder(tf.float32, shape=[None, n_classes])\n",
    "\n",
    "# Store layers' weights & biases\n",
    "weights = {\n",
    "    'h': tf.Variable(tf.random_normal([n_input, n_hidden], stddev = np.sqrt(1.0/n_input) )),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes], stddev = np.sqrt(1.0/n_hidden) ))\n",
    "}\n",
    "biases = {\n",
    "    'b': tf.Variable(tf.random_normal([n_hidden], stddev = np.sqrt(1.0/n_input) )),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes], stddev = np.sqrt(1.0/n_hidden) ))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "logits = neural_network(tf_train_dataset, weights, biases)\n",
    "\n",
    "# Define cost\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, targets=tf_train_labels)) # Match pred with y\n",
    "\n",
    "# Make prediction\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "valid_prediction = tf.nn.softmax(neural_network(tf_valid_dataset, weights, biases))\n",
    "test_prediction = tf.nn.softmax(neural_network(tf_test_dataset, weights, biases))\n",
    "\n",
    "# Compare prediction with labels to evaluate accuracy\n",
    "correct_preds = tf.equal(tf.argmax(pred, 1), tf.argmax(label, 1))\n",
    "accuracy_rate = tf.reduce_mean(tf.cast(correct_preds, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def part_2(learning_rate, batch_size, num_epochs):\n",
    "    train_op = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)# momentum=momentum)\n",
    "    #train_op = optimizer.minimize(cost)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    # Launch the graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        # Training cycle\n",
    "        for epoch in range(num_epochs):\n",
    "            avg_cost = 0.\n",
    "            num_batches = int(training_set_size/batch_size)\n",
    "            # Loop over all batches\n",
    "            for i in range(num_batches):\n",
    "                batch_xs = training_images[i * batch_size: (i + 1) * batch_size]\n",
    "                batch_ys = training_labels[i * batch_size: (i + 1) * batch_size]\n",
    "                feed_dict = {tf_train_dataset : batch_xs, tf_train_labels : batch_ys}\n",
    "                _, l, train_preds = sess.run([train_op, cost, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "            # Display per epoch \n",
    "            if (epoch % display_step) == 0:\n",
    "                train_accuracy, train_hits = sess.run([accuracy_rate, correct_preds], feed_dict={pred: train_preds, label: batch_ys})\n",
    "                valid_accuracy, valid_hits  = sess.run([accuracy_rate, correct_preds], feed_dict={pred: valid_prediction.eval(), label: validation_labels})\n",
    "                print \"Epoch:\", '%04d' % (epoch+1)\n",
    "                print(\"Minibatch accuracy: %.5f\" % train_accuracy)\n",
    "                print(\"Validation Accuracy: %.5f\" % valid_accuracy)\n",
    "        test_accuracy, test_hits  = sess.run([accuracy_rate, correct_preds], feed_dict={pred: test_prediction.eval(), label: testing_labels})\n",
    "        print \"Training Completed\"\n",
    "        print(\"Testing Accuracy: %.5f\" % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "part_2(0.5, 1000, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "part_2(0.1, 1000, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "part_2(0.05, 1000, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "part_2(0.01, 1000, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "part_2(0.001, 1000, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "f = open('NN - learning rate = %f.txt' % learning_rate, 'a')\n",
    "f.write(training_accuracies)\n",
    "f.write('\\n')\n",
    "f.write(validation_accuracies)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#batch_ys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_preds[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
